{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b445c093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93eefa57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1f762a9d2742978cb2f2ab3af49fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = r\"d:\\models\\llama8b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f663d0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(128256, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define LoRA adapter config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # LLaMA convention\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Attach LoRA adapters\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39e8e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx][\"input_text\"]\n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = encodings[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = encodings[\"attention_mask\"].squeeze(0)\n",
    "        # Labels = input_ids for self-supervised LM\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7426b5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt template all the mail contents are passed onto the prompt \n",
    "\n",
    "\n",
    "prompt_template =\"\"\"\n",
    "You are a cargo email information extraction assistant. Your job is to extract structured JSON data from a single airline cargo-related email.\n",
    "Subject: {subject}\n",
    "From: {from_}\n",
    "To: {to}\n",
    "Body:{body}\n",
    "Your task is to extract only the entities listed below from this one email. Output a clean, valid **JSON array of dictionaries**, where each dictionary corresponds to one AWB (Air Waybill). Do **not** generate anything outside the JSON.\n",
    "Each dictionary should contain only the following fields:\n",
    "[\n",
    "    {{\n",
    "        \"AWB\": \"\",\n",
    "        \"FlightNo\": \"\",\n",
    "        \"Departure-date\": \"\",\n",
    "        \"total-pieces\": ,\n",
    "        \"pieces@dimensions\": [\"\"],\n",
    "        \"dimension-unit\": [\"\"],\n",
    "        \"Weight\": ,\n",
    "        \"weight-unit\": \"\",\n",
    "        \"special-instruction\": \"\",\n",
    "        \"commodity-description\": \"\",\n",
    "        \"product-code\": \"\",\n",
    "        \"Source\": \"\",\n",
    "        \"Destination\": \"\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "### EXTRACTION RULES:\n",
    "AWB (Air Waybill):\n",
    "Must be 11-digit numbers starting with valid airline prefixes: <AWB_PREFIX>.\n",
    "May be referred to as \"MAWB\" or \"GUIA\".\n",
    "Remove hyphens or spaces.\n",
    "One dictionary per AWB; multiple AWBs = multiple dictionaries.\n",
    " \n",
    "FlightNo:\n",
    "Must start with valid carrier codes: <AIRLINE_PREFIX>.\n",
    "Format: airline code + number\n",
    "do not take the date value for the flight number if there is flight date attached with flight number (eg KE706/18APR in this only take KE706)\n",
    "if no values are found keep as null\n",
    " \n",
    "Departure-date:\n",
    "Extract in YYYY-MM-DD format.\n",
    "If given as a range like 23/24/07, choose the latest date (i.e., 2025-07-24).\n",
    "If given as a relative day (e.g., \"next Monday\"), assume today's date is 2025-03-22 (Saturday) and resolve accordingly.\n",
    " \n",
    "total-pieces:\n",
    "Integer value representing total cargo pieces.\n",
    " \n",
    "pieces@dimensions:\n",
    "Format: list like [\"2@24x17x9\"].\n",
    "May appear as pcs x l x b x h or pcs @ l x b x h.\n",
    "Extract all combinations; prioritize individual dimensions over total.\n",
    " \n",
    "dimension-unit:\n",
    "Supported units: \"CM\", \"M\", \"IN\", \"OTH\".\n",
    "Provide as a list matching the sequence of pieces@dimensions.\n",
    " \n",
    "Weight:\n",
    "If individual weights are given, compute the total.\n",
    "Use chargeable weight (CW) or gross weight (G/W) if explicitly mentioned.\n",
    "If weight is embedded in piece-dimension combos, extract accordingly.\n",
    " \n",
    "weight-unit:\n",
    "Supported values: \"KG\", \"KGS\", \"LBS\", \"OTH\".\n",
    " \n",
    "special-instruction:\n",
    "Extract any special handling notes.\n",
    "Always translate to English.\n",
    " \n",
    "commodity-description:\n",
    "Free text describing the goods.\n",
    "Always translate to English.\n",
    " \n",
    "product-code:\n",
    "If not explicitly given, infer from commodity description:\n",
    "\"GEN\" for general cargo\n",
    "\"HAZ\" for hazardous materials\n",
    "\"DG\" for dangerous goods\n",
    " \n",
    "Source / Destination:\n",
    "Extract from IATA codes in formats like EWR-OME (EWR = Source, OME = Destination).\n",
    "Do not assume source location from sender's location or from flight number.\n",
    "\n",
    "### EXTRACTION RULES:\n",
    "- Extract **only for the single email above**.\n",
    "- Output must include only fields you found.\n",
    "- If a field is missing, return null (no guessing).\n",
    "- Do NOT include any other text, explanation, markdown, or tags.\n",
    "- Output must be valid JSON and must **end after the first array**.\n",
    "\n",
    "### STOP CONDITIONS:\n",
    "- ❌ Do NOT repeat the JSON.\n",
    "- ❌ Do NOT generate for other emails.\n",
    "- ✅ Your response must end immediately after the closing square bracket `]`.\n",
    "- ✅ Append `### END JSON` after the closing bracket to indicate completion.\n",
    "- ⚠️ If your generation continues after that, it will be rejected.\n",
    "\n",
    "Now return only the extracted JSON output for the above email.\n",
    "ABSOLUTLEY NO EXPLANATION\n",
    "OMIT ALL EXPLANATIONS AND FORCE STOP GENERATION AFTER JSON FILES ARE GENERATED\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1c8ba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(r\"D:\\crf\\processed_emails_with_ground_truth.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "# Preprocess\n",
    "processed = []\n",
    "for item in raw_data:\n",
    "    subject = item.get(\"subject\", \"\")\n",
    "    body = item.get(\"body\", \"\")\n",
    "    sender = item.get(\"from\", \"\")\n",
    "    receiver = item.get(\"to\", \"\")\n",
    "    text = f\"{prompt_template} Subject: {subject} | Body: {body} | From: {sender} | To: {receiver}\"\n",
    "    processed.append({\"input_text\": text})\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "dataset = EmailDataset(processed, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33357a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1261628c51194bf2be6a1df9bc12d950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/148 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Convert your processed list of dicts to a HuggingFace dataset\n",
    "hf_dataset = Dataset.from_list(processed)\n",
    "\n",
    "# Define tokenizer function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"input_text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# Apply tokenizer to the dataset\n",
    "tokenized_ds = hf_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Optional: set format to PyTorch tensors\n",
    "tokenized_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "492c7e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a2b15b31e34a3bb79f497d1b9848a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/148 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='222' max='222' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [222/222 01:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.772600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.250600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.602700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.882300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.487300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.308300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.208200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.103900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.058400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.049400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.042300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.040800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.040700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.039000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.038400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.038700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.037700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.038300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.037300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.036400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.036400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=222, training_loss=0.414139936434793, metrics={'train_runtime': 87.5565, 'train_samples_per_second': 5.071, 'train_steps_per_second': 2.536, 'total_flos': 2561443387932672.0, 'train_loss': 0.414139936434793, 'epoch': 3.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For causal LM, labels = input_ids\n",
    "def add_labels(batch):\n",
    "    batch[\"labels\"] = batch[\"input_ids\"].clone()\n",
    "    return batch\n",
    "\n",
    "tokenized_ds = tokenized_ds.map(add_labels)\n",
    "\n",
    "tokenized_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,  # if using GPU\n",
    ")\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Replace 'model_name' with your model checkpoint\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"model\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3fb68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./8b_adptv2\")\n",
    "tokenizer.save_pretrained(\"./8b_adptv2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
